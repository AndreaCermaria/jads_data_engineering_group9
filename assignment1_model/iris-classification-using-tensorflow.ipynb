{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction model of type of iris plant via Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "30e84fc2-0457-d60c-38c4-b7681d2e75a6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 12:14:17.587061: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-24 12:14:17.676668: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-24 12:14:17.680086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:17.680096: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-24 12:14:17.697029: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-24 12:14:18.033281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.033315: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.033319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sources:\n",
    "https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "https://wellsr.com/python/python-tensorflow-2-for-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "dataset = sns.load_dataset('iris')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output_setosa</th>\n",
       "      <th>output_versicolor</th>\n",
       "      <th>output_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     output_setosa  output_versicolor  output_virginica\n",
       "0                1                  0                 0\n",
       "1                1                  0                 0\n",
       "2                1                  0                 0\n",
       "3                1                  0                 0\n",
       "4                1                  0                 0\n",
       "..             ...                ...               ...\n",
       "145              0                  0                 1\n",
       "146              0                  0                 1\n",
       "147              0                  0                 1\n",
       "148              0                  0                 1\n",
       "149              0                  0                 1\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# divide dependent and independent variable\n",
    "X = dataset.drop(['species'], axis=1)\n",
    "y = pd.get_dummies(dataset.species, prefix='output')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.values\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.6, 3.4, 1.4, 0.3],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [5.5, 2.6, 4.4, 1.2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train - test split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=39)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed for now! Don't uncomment\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 12:14:18.734986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-24 12:14:18.735636: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.735771: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.735886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.735991: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.736092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.736200: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.736306: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.736405: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-24 12:14:18.736426: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-24 12:14:18.737544: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "input_1 = Input(shape=(X_train.shape[1],))\n",
    "l1 = Dense(100, activation='relu')(input_1)\n",
    "l2 = Dense(50, activation='relu')(l1)\n",
    "l3 = Dense(25, activation='relu')(l2)\n",
    "output_1 = Dense(y_train.shape[1], activation='softmax')(l3)\n",
    "\n",
    "model = Model(inputs = input_1, outputs = output_1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 1s 14ms/step - loss: 1.2133 - acc: 0.5625 - val_loss: 0.9790 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9292 - acc: 0.6562 - val_loss: 0.8477 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.7659 - acc: 0.6771 - val_loss: 0.7302 - val_acc: 0.8333\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6142 - acc: 0.8333 - val_loss: 0.6346 - val_acc: 0.6250\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5147 - acc: 0.8333 - val_loss: 0.5510 - val_acc: 0.6667\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4456 - acc: 0.8750 - val_loss: 0.5160 - val_acc: 0.6667\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3890 - acc: 0.9375 - val_loss: 0.4811 - val_acc: 0.6667\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3452 - acc: 0.9479 - val_loss: 0.3748 - val_acc: 0.9167\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2804 - acc: 0.9688 - val_loss: 0.3864 - val_acc: 0.7917\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2378 - acc: 0.9375 - val_loss: 0.3880 - val_acc: 0.7917\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2143 - acc: 0.9583 - val_loss: 0.4602 - val_acc: 0.6667\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1834 - acc: 0.9688 - val_loss: 0.2769 - val_acc: 0.9167\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1675 - acc: 0.9479 - val_loss: 0.3100 - val_acc: 0.8333\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1510 - acc: 0.9583 - val_loss: 0.2061 - val_acc: 0.9167\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1193 - acc: 0.9583 - val_loss: 0.2451 - val_acc: 0.9167\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1090 - acc: 0.9583 - val_loss: 0.2119 - val_acc: 0.9167\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1188 - acc: 0.9688 - val_loss: 0.1537 - val_acc: 0.9583\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0976 - acc: 0.9688 - val_loss: 0.1751 - val_acc: 0.9583\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0929 - acc: 0.9688 - val_loss: 0.1738 - val_acc: 0.9167\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1045 - acc: 0.9688 - val_loss: 0.3003 - val_acc: 0.8333\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=4, epochs=20, verbose=1, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0973 - acc: 0.9667\n",
      "Test Accuracy: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.96100545e-01, 3.89944669e-03, 2.73981660e-09, 9.93250251e-01,\n",
       "       6.74974173e-03, 5.96481886e-09, 1.33020842e-06, 1.19032292e-02,\n",
       "       9.88095462e-01, 5.10520022e-03, 9.67584014e-01, 2.73107719e-02,\n",
       "       9.93791401e-01, 6.20865868e-03, 1.07821112e-08, 9.93804514e-01,\n",
       "       6.19542645e-03, 4.03691081e-09, 9.33732565e-07, 9.26996209e-03,\n",
       "       9.90729094e-01, 9.92412031e-01, 7.58789806e-03, 9.73922454e-09,\n",
       "       3.92924562e-07, 7.19960080e-03, 9.92799997e-01, 3.04772448e-06,\n",
       "       7.92910457e-02, 9.20705855e-01, 6.89177075e-04, 5.35943449e-01,\n",
       "       4.63367432e-01, 9.97474730e-01, 2.52525532e-03, 5.10407216e-10,\n",
       "       3.91955473e-06, 4.06962186e-02, 9.59299803e-01, 1.55505482e-02,\n",
       "       9.64398324e-01, 2.00510658e-02, 1.31233782e-03, 9.24529731e-01,\n",
       "       7.41579458e-02, 2.39793700e-03, 9.03488755e-01, 9.41133425e-02,\n",
       "       1.63909283e-06, 2.87260469e-02, 9.71272230e-01, 4.02583993e-07,\n",
       "       1.04523832e-02, 9.89547133e-01, 5.62060961e-07, 8.39887466e-03,\n",
       "       9.91600573e-01, 9.94273007e-01, 5.72696794e-03, 4.19160351e-09,\n",
       "       2.27156324e-05, 1.05969235e-01, 8.94008040e-01, 9.98643577e-01,\n",
       "       1.35642372e-03, 2.19210428e-10, 9.89297628e-01, 1.07023874e-02,\n",
       "       1.52173971e-08, 5.02801174e-03, 8.33344996e-01, 1.61626890e-01,\n",
       "       3.32254474e-03, 9.29787695e-01, 6.68897629e-02, 9.96508777e-01,\n",
       "       3.49120074e-03, 1.23709365e-09, 1.60467880e-05, 2.41163056e-02,\n",
       "       9.75867629e-01, 4.79878625e-04, 5.50386429e-01, 4.49133724e-01,\n",
       "       2.91328706e-06, 1.72260981e-02, 9.82770979e-01, 6.54240313e-04,\n",
       "       4.55764174e-01, 5.43581545e-01], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model.predict(X_test).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model in .h5 format\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               500       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 78        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,903\n",
      "Trainable params: 6,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the model to test\n",
    "model = load_model('model.h5')\n",
    "# summarize model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "[[1.6046788e-05 2.4116306e-02 9.7586763e-01]] [[9.9255204e-01 7.4479925e-03 1.1362550e-08]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Virginica', 'Setosa')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data\n",
    "data1 = pd.DataFrame({\"Sepal.Length\": [4.9],\"Sepal.Width\": [2.5],\"Petal.Length\": [4.5],\"Petal.Width\": [1.7]})\n",
    "data2 = pd.DataFrame({\"Sepal.Length\": [4.6],\"Sepal.Width\": [3.1],\"Petal.Length\": [1.5],\"Petal.Width\": [0.2]})\n",
    "\n",
    "y_pred1 = model.predict(data1)\n",
    "y_pred2 = model.predict(data2)\n",
    "species = {0:'Setosa',1:'Versicolor',2:'Virginica' }\n",
    "print(y_pred1,y_pred2)\n",
    "species[np.argmax(y_pred1)],species[np.argmax(y_pred2)]"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
